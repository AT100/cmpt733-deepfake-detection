{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f360b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd45772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e33961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto(gpu_options = \n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset/train_images'\n",
    "val_dir = 'dataset/val_images'\n",
    "test_dir = 'dataset/test_images'\n",
    "train_dir2 = 'train'\n",
    "\n",
    "#datagen = ImageDataGenerator(1./255)\n",
    "train_generator = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    image_size=(256,256),\n",
    "    seed=123,\n",
    "    color_mode='rgb',\n",
    "    subset = 'training'\n",
    ")\n",
    "val_generator = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    image_size=(256,256),\n",
    "    seed=123,\n",
    "    color_mode='rgb',\n",
    "    subset = 'validation'\n",
    ")\n",
    "print(train_generator.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47225a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module(layer_in, f1,f2_in,f2_out,f3_in,f3_out,f4_out):\n",
    "    #1x1\n",
    "    conv1 = Conv2D(f1,(1,1),padding='same',activation='relu')(layer_in)\n",
    "    \n",
    "    #1x1 with 3x3\n",
    "    tower_1 = Conv2D(f2_in,(1,1),padding='same',activation='relu')(layer_in)\n",
    "    conv3 = Conv2D(f2_out,(3,3),padding='same',activation='relu')(tower_1)\n",
    "    \n",
    "    #1x1 with 5x5\n",
    "    tower_2 = Conv2D(f3_in,(1,1),padding='same',activation='relu')(layer_in)\n",
    "    conv5 = Conv2D(f3_out,(3,3),padding='same',activation='relu')(tower_2)\n",
    "    conv5 = Conv2D(f3_out,(3,3),padding='same',activation='relu')(conv5)\n",
    "    \n",
    "    #3x3 max pooling with \n",
    "    tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    conv1_2 = Conv2D(f4_out, (1,1),padding='same',activation='relu')(tower_3)\n",
    "    \n",
    "    layer_out = tf.concat([conv1,conv3,conv5,conv1_2], axis=-1)\n",
    "    \n",
    "    return layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97716b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(256,256,3))\n",
    "\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(visible)\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "\n",
    "layer = inception_module(x,64,96,128,16,32,32)\n",
    "layer = inception_module(layer, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "#x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(layer)\n",
    "#x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "\"\"\"x = MaxPooling2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size =2, strides =2, padding ='same')(x)\"\"\"\n",
    "\n",
    "\"\"\"x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size =2, strides =2, padding ='same')(x)\"\"\"\n",
    "\n",
    "#flat = Flatten()(x)\n",
    "\"\"\"out = Dense(units=256,activation='relu')(flat)\n",
    "out = Dense(units=128,activation='relu')(flat)\n",
    "out = Dense(units=64,activation='relu')(flat)\"\"\"\n",
    "\n",
    "out = GlobalAveragePooling2D()(layer)\n",
    "out = Flatten()(out)\n",
    "out = Dropout(0.2)(out)\n",
    "out = Dense(2,activation='softmax')(out)\n",
    "model = Model(inputs=visible,outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e72375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5e46a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    validation_data = val_generator,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67616dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(train_generator,\n",
    "                    validation_data = val_generator,\n",
    "                    epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "img = image.load_img('dataset/test_images/fake/df2.jpg', target_size=(256,256))\n",
    "img_1 = Image.open('dataset/test_images/fake/df2.jpg')\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "img2 = image.load_img('dataset/test_images/real/real00240.jpg', target_size=(256,256))\n",
    "img_2 = Image.open('dataset/test_images/real/real00240.jpg')\n",
    "img2 = image.img_to_array(img2)\n",
    "img2 = np.expand_dims(img2, axis = 0)\n",
    "\n",
    "classes = ['fake','real']\n",
    "\n",
    "#print(classes[np.argmax(model.predict(img))])\n",
    "#print(classes[np.argmax(model.predict(img2))])\n",
    "\n",
    "plt.imshow(img_1)\n",
    "plt.title(classes[np.argmax(model.predict(img))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f18409",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_2)\n",
    "plt.title(classes[np.argmax(model.predict(img2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(40)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dir = 'test'\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=64,\n",
    "    image_size=(256,256),\n",
    "    seed=123,\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "score = model.evaluate(test_generator)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07085e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('model/vgg16/vgg16_4.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ad147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img('dataset/test_images/fake/df2.jpg',target_size=(256,256))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img,axis=0)\n",
    "img /= 255.\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060066ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "top4_layers = [x.output for x in model.layers[-6:]]\n",
    "top4_model = Model(inputs=model.input,outputs=top4_layers)\n",
    "\n",
    "pre = top4_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f1 = pre[0]\n",
    "print(f1.shape)\n",
    "plt.matshow(f1[0,:,:,1],cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de3cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[-6:]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, pre):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
