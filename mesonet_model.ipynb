{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGWIDTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__():\n",
    "        self.model = 0\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self.model.train_on_batch(x, y)\n",
    "    \n",
    "    def get_accuracy(self, x, y):\n",
    "        return self.model.test_on_batch(x, y)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meso1(Classifier):\n",
    "    \"\"\"\n",
    "    Feature extraction + Classification\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate = 0.001, dl_rate = 1):\n",
    "        self.model = self.init_model(dl_rate)\n",
    "        optimizer = Adam(lr = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "    \n",
    "    def init_model(self, dl_rate):\n",
    "        x = Input(shape = (IMGWIDTH, IMGWIDTH, 3))\n",
    "        \n",
    "        x1 = Conv2D(16, (3, 3), dilation_rate = dl_rate, strides = 1, padding='same', activation = 'relu')(x)\n",
    "        x1 = Conv2D(4, (1, 1), padding='same', activation = 'relu')(x1)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling2D(pool_size=(8, 8), padding='same')(x1)\n",
    "\n",
    "        y = Flatten()(x1)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation = 'sigmoid')(y)\n",
    "        return KerasModel(inputs = x, outputs = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meso4(Classifier):\n",
    "    def __init__(self, learning_rate = 0.001):\n",
    "        self.model = self.init_model()\n",
    "        optimizer = Adam(lr = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "    \n",
    "    def init_model(self): \n",
    "        x = Input(shape = (IMGWIDTH, IMGWIDTH, 3))\n",
    "        \n",
    "        x1 = Conv2D(8, (3, 3), padding='same', activation = 'relu')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "        \n",
    "        x2 = Conv2D(8, (5, 5), padding='same', activation = 'relu')(x1)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n",
    "        \n",
    "        x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n",
    "        x3 = BatchNormalization()(x3)\n",
    "        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "        \n",
    "        x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "        \n",
    "        y = Flatten()(x4)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(16)(y)\n",
    "        y = LeakyReLU(alpha=0.1)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation = 'sigmoid')(y)\n",
    "\n",
    "        return KerasModel(inputs = x, outputs = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoInception4(Classifier):\n",
    "    def __init__(self, learning_rate = 0.001):\n",
    "        self.model = self.init_model()\n",
    "        optimizer = Adam(lr = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "    \n",
    "    def InceptionLayer(self, a, b, c, d):\n",
    "        def func(x):\n",
    "            x1 = Conv2D(a, (1, 1), padding='same', activation='relu')(x)\n",
    "            \n",
    "            x2 = Conv2D(b, (1, 1), padding='same', activation='relu')(x)\n",
    "            x2 = Conv2D(b, (3, 3), padding='same', activation='relu')(x2)\n",
    "            \n",
    "            x3 = Conv2D(c, (1, 1), padding='same', activation='relu')(x)\n",
    "            x3 = Conv2D(c, (3, 3), dilation_rate = 2, strides = 1, padding='same', activation='relu')(x3)\n",
    "            \n",
    "            x4 = Conv2D(d, (1, 1), padding='same', activation='relu')(x)\n",
    "            x4 = Conv2D(d, (3, 3), dilation_rate = 3, strides = 1, padding='same', activation='relu')(x4)\n",
    "\n",
    "            y = Concatenate(axis = -1)([x1, x2, x3, x4])\n",
    "            \n",
    "            return y\n",
    "        return func\n",
    "    \n",
    "    def init_model(self):\n",
    "        x = Input(shape = (IMGWIDTH, IMGWIDTH, 3))\n",
    "        \n",
    "        x1 = self.InceptionLayer(1, 4, 4, 2)(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "        \n",
    "        x2 = self.InceptionLayer(2, 4, 4, 2)(x1)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)        \n",
    "        \n",
    "        x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n",
    "        x3 = BatchNormalization()(x3)\n",
    "        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "        \n",
    "        x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "        \n",
    "        y = Flatten()(x4)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(16)(y)\n",
    "        y = LeakyReLU(alpha=0.1)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation = 'sigmoid')(y)\n",
    "\n",
    "        return KerasModel(inputs = x, outputs = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load the model and its pretrained weights\n",
    "# classifier = Meso4()\n",
    "# classifier.load('dataset/Meso4_DF.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 -  Create and train model:\n",
    "model = Meso4()\n",
    "if continue_train:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    #Train the model using multiple GPUs\n",
    "    #model = nn.DataParallel(model)\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    iteration = 0\n",
    "    for epoch in range(epoches):\n",
    "        print('Epoch {}/{}'.format(epoch+1, epoches))\n",
    "        print('-'*10)\n",
    "        model=model.train()\n",
    "        train_loss = 0.0\n",
    "        train_corrects = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0.0\n",
    "        for (image, labels) in train_loader:\n",
    "            iter_loss = 0.0\n",
    "            iter_corrects = 0.0\n",
    "            image = image.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_loss = loss.data.item()\n",
    "            train_loss += iter_loss\n",
    "            iter_corrects = torch.sum(preds == labels.data).to(torch.float32)\n",
    "            train_corrects += iter_corrects\n",
    "            iteration += 1\n",
    "            if not (iteration % 20):\n",
    "                print('iteration {} train loss: {:.4f} Acc: {:.4f}'.format(iteration, iter_loss / batch_size, iter_corrects / batch_size))\n",
    "        epoch_loss = train_loss / train_dataset_size\n",
    "        epoch_acc = train_corrects / train_dataset_size\n",
    "        print('epoch train loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (image, labels) in val_loader:\n",
    "                image = image.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(image)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.data.item()\n",
    "                val_corrects += torch.sum(preds == labels.data).to(torch.float32)\n",
    "            epoch_loss = val_loss / val_dataset_size\n",
    "            epoch_acc = val_corrects / val_dataset_size\n",
    "            print('epoch val loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "        scheduler.step()\n",
    "        if not (epoch % 10):\n",
    "        #Save the model trained with multiple gpu\n",
    "        #torch.save(model.module.state_dict(), os.path.join(output_path, str(epoch) + '_' + model_name))\n",
    "            torch.save(model.state_dict(), os.path.join(output_path, str(epoch) + '_' + model_name))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    #torch.save(model.module.state_dict(), os.path.join(output_path, \"best.pkl\"))\n",
    "    torch.save(model.state_dict(), os.path.join(output_path, \"best.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fake': 0, 'real': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 - Minimial image generator\n",
    "# We did use it to read and compute the prediction by batchs on test videos\n",
    "# but do as you please, the models were trained on 256x256 images in [0,1]^(n*n)\n",
    "\n",
    "dataGenerator = ImageDataGenerator(rescale=1./255)\n",
    "generator = dataGenerator.flow_from_directory(\n",
    "        'dataset/test_images',\n",
    "        target_size=(256, 256),\n",
    "        batch_size=1,\n",
    "        class_mode='binary',\n",
    "        subset='training')\n",
    "generator.class_indices\n",
    "\n",
    "# there should only be 2 classes, if not run the below to get rid or find the weird file:\n",
    "# import os\n",
    "# dir = os.listdir('dataset/test_images')\n",
    "# print(dir)\n",
    "# file_path = ('dataset/test_images/.ipynb_checkpoints')\n",
    "# os.rmdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted : [[0.3400606]] \n",
      "Real class : [0.]\n"
     ]
    }
   ],
   "source": [
    "# 3 - Predict\n",
    "X, y = generator.next()\n",
    "# print(f\"Prediction likelihood : {classifier.predict(X)[0][0]:.4f}\")\n",
    "# print(f'Actual Label: {int(y[0])}')\n",
    "# print(f\"\\nCorrect prediction: {round(classifier.predict(X)[0][0]) == y[0]}\")\n",
    "print('Predicted :', model.predict(X), '\\nReal class :', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
